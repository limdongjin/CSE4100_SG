{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02.XOR.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"ydqJyPtgWdbE","colab_type":"text"},"cell_type":"markdown","source":["# Import modules"]},{"metadata":{"id":"pISfay6wV_JP","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QyXb7pyHW1Kv","colab_type":"text"},"cell_type":"markdown","source":["# Sigmoid function and derivative\n"]},{"metadata":{"id":"9j2KPLowW0gP","colab_type":"code","colab":{}},"cell_type":"code","source":["# Sigmoid Function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Derivative of Sigmoid Function\n","def derivative_sigmoid(x):\n","    return x * (1 - x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FGXr8EGwXpCd","colab_type":"text"},"cell_type":"markdown","source":["# Data"]},{"metadata":{"id":"z7AnP3dHW_h7","colab_type":"code","colab":{}},"cell_type":"code","source":["# input\n","x = np.array([[1,1], [1,-1], [-1,-1], [-1,1]])\n","# output\n","y = np.array([[0], [1], [0], [1]])\n","\n","# plot the training data\n","fig, ax = plt.subplots()\n","for i in range(y.shape[0]):\n","  if y[i][0] == 0:\n","    marker = 'ro'\n","  else:\n","    marker = 'bo'\n","  ax.plot(x[i][0], x[i][1], marker)\n","ax.axhline(y=0, color='k')\n","ax.axvline(x=0, color='k')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BvV9bv2qXNnK","colab_type":"text"},"cell_type":"markdown","source":["# With single layer perceptron"]},{"metadata":{"id":"9W82FKBHXsIM","colab_type":"text"},"cell_type":"markdown","source":["## Layer"]},{"metadata":{"id":"dXkfCZFgXQl8","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch = 5000 # number of training iterations\n","learning_rate = 0.1\n","\n","# dimension of each layer\n","d_in = x.shape[1] # number of features in the input dataset\n","d_out = 1 # output layer\n","\n","# weight and bias initialization\n","wout = np.random.uniform(size=(d_in, 1))\n","bout = np.random.uniform(size=(1, d_out))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qvHIvWNuXx5f","colab_type":"text"},"cell_type":"markdown","source":["## Training"]},{"metadata":{"id":"gaDMRM_TXSMM","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(epoch):\n","    # Forward pass\n","    y_pred = sigmoid(x.dot(wout) + bout)\n","    \n","    # Compute and print loss\n","    loss = np.square(y_pred - y)\n","    if i % 500 == 0:\n","        print('Epoch', i, ':', loss.sum())\n","\n","    # Backpropagation to compute gradients\n","    grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)\n","    grad_wout = x.T.dot(grad_y_pred)\n","    grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)\n","    \n","    # Update weights and biases\n","    wout += grad_wout * learning_rate\n","    bout += grad_bout * learning_rate"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"qtE6YjIIZRuD"},"cell_type":"markdown","source":["## Prediction"]},{"metadata":{"id":"d1hHQlhLZWAI","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Input')\n","print(x)\n","print('Label')\n","print(y)\n","print('Output')\n","print(y_pred)\n","print('Weight')\n","print(wout)\n","print('Bias')\n","print(bout)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7eJATdHvXYYf","colab_type":"text"},"cell_type":"markdown","source":["# With multi layer perceptron"]},{"metadata":{"colab_type":"text","id":"dsv4Q8CjZmJP"},"cell_type":"markdown","source":["## Layer"]},{"metadata":{"id":"1WaScNUiXXLG","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch = 5000 # number of training iterations\n","learning_rate = 0.1\n","\n","# dimension of each layer\n","d_in = x.shape[1] # number of features in the input dataset\n","d_h = 2   # hidden layer\n","d_out = 1 # output layer\n","\n","# weight and bias initialization\n","wh = np.random.uniform(size=(d_in, d_h))\n","bh = np.random.uniform(size=(1, d_h))\n","wout = np.random.uniform(size=(d_h, d_out))\n","bout = np.random.uniform(size=(1, d_out))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"aRiSUlLuZnxf"},"cell_type":"markdown","source":["## Training"]},{"metadata":{"id":"knu_x5aqXbdc","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(epoch):\n","    # Forward pass\n","    h = sigmoid(x.dot(wh) + bh)\n","    y_pred = sigmoid(h.dot(wout) + bout)\n","    \n","    # Compute and print loss\n","    loss = np.square(y_pred - y)\n","    if i % 500 == 0:\n","        print('Epoch', i, ':', loss.sum())\n","\n","    # Backpropagation to compute gradients\n","    grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)\n","    grad_wout = h.T.dot(grad_y_pred)\n","    grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)\n","    grad_h = grad_y_pred.dot(wout.T) * derivative_sigmoid(h)\n","    grad_wh = x.T.dot(grad_h)\n","    grad_bh = np.sum(grad_h, axis=0, keepdims=True)\n","\n","    # Update weights and biases\n","    wout += grad_wout * learning_rate\n","    bout += grad_bout * learning_rate\n","    wh += grad_wh * learning_rate\n","    bh += grad_bh * learning_rate\n","    \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fcqtQVZuZtYL","colab_type":"text"},"cell_type":"markdown","source":["## Prediction"]},{"metadata":{"id":"zXvIuM_dZv_A","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Input')\n","print(x)\n","print('Label')\n","print(y)\n","print('Output')\n","print(y_pred)\n","print('Weight @ Hidden layer')\n","print(wh)\n","print('Bias @ Hidden layer')\n","print(bh)\n","print('Weight @ Output layer')\n","print(wout)\n","print('Bias @ Output layer')\n","print(bout)"],"execution_count":0,"outputs":[]}]}