{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01.AND/OR.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"hmz9uH8tHrbw","colab_type":"text"},"cell_type":"markdown","source":["# Import modules"]},{"metadata":{"id":"L52E8e-lHYsE","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JEDP54yxWRBs","colab_type":"text"},"cell_type":"markdown","source":["# Sigmoid function and derivative"]},{"metadata":{"id":"zgA4GEIVHeU6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Sigmoid Function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Derivative of Sigmoid Function\n","def derivative_sigmoid(x):\n","    return x * (1 - x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0gvKe4DuHpBd","colab_type":"text"},"cell_type":"markdown","source":["# AND Problem"]},{"metadata":{"id":"FGXr8EGwXpCd","colab_type":"text"},"cell_type":"markdown","source":["## Data"]},{"metadata":{"id":"oeBPccv6H03j","colab_type":"code","colab":{}},"cell_type":"code","source":["# input\n","x = np.array([[1,1], [1,-1], [-1,-1], [-1,1]])\n","# output\n","y = np.array([[1], [0], [0], [0]])\n","\n","# plot the training data\n","fig, ax = plt.subplots()\n","for i in range(y.shape[0]):\n","  if y[i][0] == 0:\n","    marker = 'ro'\n","  else:\n","    marker = 'bo'\n","  ax.plot(x[i][0], x[i][1], marker)\n","ax.axhline(y=0, color='k')\n","ax.axvline(x=0, color='k')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"9W82FKBHXsIM","colab_type":"text"},"cell_type":"markdown","source":["## Layer"]},{"metadata":{"id":"ZNkERUrqH2aJ","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch = 5000 # number of training iterations\n","learning_rate = 0.1\n","\n","# dimension of each layer\n","d_in = x.shape[1] # number of features in the input dataset\n","d_out = 1 # output layer\n","\n","# weight and bias initialization\n","wout = np.random.uniform(size=(d_in, 1))\n","bout = np.random.uniform(size=(1, d_out))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qvHIvWNuXx5f","colab_type":"text"},"cell_type":"markdown","source":["## Train"]},{"metadata":{"id":"S2sWeoImIBVC","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(epoch):\n","    # Forward pass\n","    y_pred = sigmoid(x.dot(wout) + bout)\n","    \n","    # Compute and print loss\n","    loss = np.square(y_pred - y)\n","    if i % 500 == 0:\n","        print('Epoch', i, ':', loss.sum())\n","\n","    # Backpropagation to compute gradients\n","    grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)\n","    grad_wout = x.T.dot(grad_y_pred)\n","    grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)\n","    \n","    # Update weights and biases\n","    wout += grad_wout * learning_rate\n","    bout += grad_bout * learning_rate"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8VIZB_WMX0tr","colab_type":"text"},"cell_type":"markdown","source":["## Prediction"]},{"metadata":{"id":"jUVes_YWVlDF","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Input')\n","print(x)\n","print('Label')\n","print(y)\n","print('Output')\n","print(y_pred)\n","print('Weight')\n","print(wout)\n","print('Bias')\n","print(bout)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KyxUqwCBNgaA","colab_type":"text"},"cell_type":"markdown","source":["# OR Problem"]},{"metadata":{"colab_type":"text","id":"MeKK67UUYr0r"},"cell_type":"markdown","source":["## Data"]},{"metadata":{"id":"91bg7rY8VRTb","colab_type":"code","colab":{}},"cell_type":"code","source":["# input\n","x = np.array([[1,1], [1,-1], [-1,-1], [-1,1]])\n","# output\n","y = np.array([[1], [1], [0], [1]])\n","\n","# plot the training data\n","fig, ax = plt.subplots()\n","for i in range(y.shape[0]):\n","  if y[i][0] == 0:\n","    marker = 'ro'\n","  else:\n","    marker = 'bo'\n","  ax.plot(x[i][0], x[i][1], marker)\n","ax.axhline(y=0, color='k')\n","ax.axvline(x=0, color='k')"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"dgjPI111Ytoo"},"cell_type":"markdown","source":["## Layer"]},{"metadata":{"id":"SBWpjvR-VUBy","colab_type":"code","colab":{}},"cell_type":"code","source":["epoch = 5000 # number of training iterations\n","learning_rate = 0.1\n","\n","# dimension of each layer\n","d_in = x.shape[1] # number of features in the input dataset\n","d_out = 1 # output layer\n","\n","# weight and bias initialization\n","wout = np.random.uniform(size=(d_in, 1))\n","bout = np.random.uniform(size=(1, d_out))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"reACrSleYvMQ"},"cell_type":"markdown","source":["## Train"]},{"metadata":{"id":"EOSgTnfWVVeC","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(epoch):\n","    # Forward pass\n","    y_pred = sigmoid(x.dot(wout) + bout)\n","    \n","    # Compute and print loss\n","    loss = np.square(y_pred - y)\n","    if i % 500 == 0:\n","        print('Epoch', i, ':', loss.sum())\n","\n","    # Backpropagation to compute gradients\n","    grad_y_pred = (y - y_pred) * derivative_sigmoid(y_pred)\n","    grad_wout = x.T.dot(grad_y_pred)\n","    grad_bout = np.sum(grad_y_pred, axis=0, keepdims=True)\n","    \n","    # Update weights and biases\n","    wout += grad_wout * learning_rate\n","    bout += grad_bout * learning_rate"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"-0J9x1BEY5lE"},"cell_type":"markdown","source":["## Prediction"]},{"metadata":{"colab_type":"code","id":"Ad3QOsLZY5lJ","colab":{}},"cell_type":"code","source":["print('Input')\n","print(x)\n","print('Label')\n","print(y)\n","print('Output')\n","print(y_pred)\n","print('Weight')\n","print(wout)\n","print('Bias')\n","print(bout)"],"execution_count":0,"outputs":[]}]}